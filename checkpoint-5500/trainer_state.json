{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.4360313315926894,
  "eval_steps": 500,
  "global_step": 5500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.013054830287206266,
      "grad_norm": 130.7843475341797,
      "learning_rate": 9.988511749347258e-07,
      "loss": 128.3369,
      "step": 50
    },
    {
      "epoch": 0.02610966057441253,
      "grad_norm": 162.8450469970703,
      "learning_rate": 9.975718015665796e-07,
      "loss": 115.0746,
      "step": 100
    },
    {
      "epoch": 0.0391644908616188,
      "grad_norm": 75.6275634765625,
      "learning_rate": 9.962924281984333e-07,
      "loss": 120.7544,
      "step": 150
    },
    {
      "epoch": 0.05221932114882506,
      "grad_norm": 110.87567901611328,
      "learning_rate": 9.949869451697128e-07,
      "loss": 118.4969,
      "step": 200
    },
    {
      "epoch": 0.06527415143603134,
      "grad_norm": 38.08506393432617,
      "learning_rate": 9.936814621409922e-07,
      "loss": 129.8019,
      "step": 250
    },
    {
      "epoch": 0.0783289817232376,
      "grad_norm": 53.04234313964844,
      "learning_rate": 9.923759791122715e-07,
      "loss": 104.6556,
      "step": 300
    },
    {
      "epoch": 0.09138381201044386,
      "grad_norm": 91.80441284179688,
      "learning_rate": 9.91070496083551e-07,
      "loss": 119.2988,
      "step": 350
    },
    {
      "epoch": 0.10443864229765012,
      "grad_norm": 54.291107177734375,
      "learning_rate": 9.897650130548302e-07,
      "loss": 126.5359,
      "step": 400
    },
    {
      "epoch": 0.1174934725848564,
      "grad_norm": 73.1610336303711,
      "learning_rate": 9.884595300261097e-07,
      "loss": 120.2493,
      "step": 450
    },
    {
      "epoch": 0.13054830287206268,
      "grad_norm": 129.36204528808594,
      "learning_rate": 9.87154046997389e-07,
      "loss": 111.3249,
      "step": 500
    },
    {
      "epoch": 0.14360313315926893,
      "grad_norm": 46.357872009277344,
      "learning_rate": 9.858485639686684e-07,
      "loss": 117.1317,
      "step": 550
    },
    {
      "epoch": 0.1566579634464752,
      "grad_norm": 67.59294128417969,
      "learning_rate": 9.845430809399479e-07,
      "loss": 115.5943,
      "step": 600
    },
    {
      "epoch": 0.16971279373368145,
      "grad_norm": 58.14019775390625,
      "learning_rate": 9.832375979112271e-07,
      "loss": 112.2912,
      "step": 650
    },
    {
      "epoch": 0.18276762402088773,
      "grad_norm": 45.20419692993164,
      "learning_rate": 9.819321148825066e-07,
      "loss": 116.1725,
      "step": 700
    },
    {
      "epoch": 0.195822454308094,
      "grad_norm": 79.6578140258789,
      "learning_rate": 9.806266318537858e-07,
      "loss": 111.9657,
      "step": 750
    },
    {
      "epoch": 0.20887728459530025,
      "grad_norm": 358.2881164550781,
      "learning_rate": 9.793211488250653e-07,
      "loss": 106.1706,
      "step": 800
    },
    {
      "epoch": 0.22193211488250653,
      "grad_norm": 68.72689056396484,
      "learning_rate": 9.780156657963446e-07,
      "loss": 107.5186,
      "step": 850
    },
    {
      "epoch": 0.2349869451697128,
      "grad_norm": 65.25933074951172,
      "learning_rate": 9.76710182767624e-07,
      "loss": 107.1409,
      "step": 900
    },
    {
      "epoch": 0.24804177545691905,
      "grad_norm": 336.9642333984375,
      "learning_rate": 9.754046997389035e-07,
      "loss": 111.3741,
      "step": 950
    },
    {
      "epoch": 0.26109660574412535,
      "grad_norm": 248.1244354248047,
      "learning_rate": 9.740992167101827e-07,
      "loss": 111.0287,
      "step": 1000
    },
    {
      "epoch": 0.2741514360313316,
      "grad_norm": 308.9012756347656,
      "learning_rate": 9.727937336814622e-07,
      "loss": 104.9568,
      "step": 1050
    },
    {
      "epoch": 0.28720626631853785,
      "grad_norm": 61.4293327331543,
      "learning_rate": 9.714882506527415e-07,
      "loss": 97.9443,
      "step": 1100
    },
    {
      "epoch": 0.3002610966057441,
      "grad_norm": 59.99623489379883,
      "learning_rate": 9.70182767624021e-07,
      "loss": 99.4737,
      "step": 1150
    },
    {
      "epoch": 0.3133159268929504,
      "grad_norm": 42.50373840332031,
      "learning_rate": 9.688772845953002e-07,
      "loss": 103.2941,
      "step": 1200
    },
    {
      "epoch": 0.3263707571801567,
      "grad_norm": 75.18800354003906,
      "learning_rate": 9.675718015665796e-07,
      "loss": 96.7853,
      "step": 1250
    },
    {
      "epoch": 0.3394255874673629,
      "grad_norm": 36.98752212524414,
      "learning_rate": 9.66266318537859e-07,
      "loss": 102.6792,
      "step": 1300
    },
    {
      "epoch": 0.3524804177545692,
      "grad_norm": 753.3485717773438,
      "learning_rate": 9.649608355091384e-07,
      "loss": 105.8134,
      "step": 1350
    },
    {
      "epoch": 0.36553524804177545,
      "grad_norm": 52.9523811340332,
      "learning_rate": 9.636553524804178e-07,
      "loss": 105.5025,
      "step": 1400
    },
    {
      "epoch": 0.3785900783289817,
      "grad_norm": 93.99546813964844,
      "learning_rate": 9.62349869451697e-07,
      "loss": 104.2262,
      "step": 1450
    },
    {
      "epoch": 0.391644908616188,
      "grad_norm": 53.4461784362793,
      "learning_rate": 9.610443864229766e-07,
      "loss": 94.1789,
      "step": 1500
    },
    {
      "epoch": 0.4046997389033943,
      "grad_norm": 449.74627685546875,
      "learning_rate": 9.597389033942558e-07,
      "loss": 93.3586,
      "step": 1550
    },
    {
      "epoch": 0.4177545691906005,
      "grad_norm": 69.26262664794922,
      "learning_rate": 9.584334203655353e-07,
      "loss": 92.7094,
      "step": 1600
    },
    {
      "epoch": 0.4308093994778068,
      "grad_norm": 40.27420425415039,
      "learning_rate": 9.571279373368145e-07,
      "loss": 89.0638,
      "step": 1650
    },
    {
      "epoch": 0.44386422976501305,
      "grad_norm": 47.72166442871094,
      "learning_rate": 9.55822454308094e-07,
      "loss": 92.1242,
      "step": 1700
    },
    {
      "epoch": 0.45691906005221933,
      "grad_norm": 58.00398635864258,
      "learning_rate": 9.545169712793735e-07,
      "loss": 93.6911,
      "step": 1750
    },
    {
      "epoch": 0.4699738903394256,
      "grad_norm": 70.7789077758789,
      "learning_rate": 9.532114882506527e-07,
      "loss": 88.8803,
      "step": 1800
    },
    {
      "epoch": 0.4830287206266319,
      "grad_norm": 44.93568420410156,
      "learning_rate": 9.519060052219321e-07,
      "loss": 92.8575,
      "step": 1850
    },
    {
      "epoch": 0.4960835509138381,
      "grad_norm": 184.04884338378906,
      "learning_rate": 9.506005221932114e-07,
      "loss": 96.0665,
      "step": 1900
    },
    {
      "epoch": 0.5091383812010444,
      "grad_norm": 40.03663635253906,
      "learning_rate": 9.492950391644908e-07,
      "loss": 96.7011,
      "step": 1950
    },
    {
      "epoch": 0.5221932114882507,
      "grad_norm": 50.82585144042969,
      "learning_rate": 9.479895561357702e-07,
      "loss": 86.9971,
      "step": 2000
    },
    {
      "epoch": 0.5352480417754569,
      "grad_norm": 63.9655876159668,
      "learning_rate": 9.466840731070495e-07,
      "loss": 91.3411,
      "step": 2050
    },
    {
      "epoch": 0.5483028720626631,
      "grad_norm": 38.75367736816406,
      "learning_rate": 9.45378590078329e-07,
      "loss": 88.5923,
      "step": 2100
    },
    {
      "epoch": 0.5613577023498695,
      "grad_norm": 89.17598724365234,
      "learning_rate": 9.440731070496083e-07,
      "loss": 84.5557,
      "step": 2150
    },
    {
      "epoch": 0.5744125326370757,
      "grad_norm": 39.80143356323242,
      "learning_rate": 9.427676240208877e-07,
      "loss": 86.8893,
      "step": 2200
    },
    {
      "epoch": 0.587467362924282,
      "grad_norm": 87.19153594970703,
      "learning_rate": 9.414621409921671e-07,
      "loss": 86.6214,
      "step": 2250
    },
    {
      "epoch": 0.6005221932114883,
      "grad_norm": 41.99812698364258,
      "learning_rate": 9.401566579634464e-07,
      "loss": 84.5413,
      "step": 2300
    },
    {
      "epoch": 0.6135770234986945,
      "grad_norm": 97.59964752197266,
      "learning_rate": 9.388511749347258e-07,
      "loss": 91.3072,
      "step": 2350
    },
    {
      "epoch": 0.6266318537859008,
      "grad_norm": 74.16480255126953,
      "learning_rate": 9.375456919060051e-07,
      "loss": 95.9636,
      "step": 2400
    },
    {
      "epoch": 0.639686684073107,
      "grad_norm": 85.68753051757812,
      "learning_rate": 9.362402088772845e-07,
      "loss": 82.6047,
      "step": 2450
    },
    {
      "epoch": 0.6527415143603134,
      "grad_norm": 32.68789291381836,
      "learning_rate": 9.34934725848564e-07,
      "loss": 87.7314,
      "step": 2500
    },
    {
      "epoch": 0.6657963446475196,
      "grad_norm": 52.36418914794922,
      "learning_rate": 9.336292428198433e-07,
      "loss": 80.8949,
      "step": 2550
    },
    {
      "epoch": 0.6788511749347258,
      "grad_norm": 50.1995964050293,
      "learning_rate": 9.323237597911227e-07,
      "loss": 84.8148,
      "step": 2600
    },
    {
      "epoch": 0.6919060052219321,
      "grad_norm": 37.54075241088867,
      "learning_rate": 9.31018276762402e-07,
      "loss": 85.3158,
      "step": 2650
    },
    {
      "epoch": 0.7049608355091384,
      "grad_norm": 308.8097229003906,
      "learning_rate": 9.297127937336814e-07,
      "loss": 76.5796,
      "step": 2700
    },
    {
      "epoch": 0.7180156657963447,
      "grad_norm": 87.5346908569336,
      "learning_rate": 9.284073107049608e-07,
      "loss": 74.1329,
      "step": 2750
    },
    {
      "epoch": 0.7310704960835509,
      "grad_norm": 64.65233612060547,
      "learning_rate": 9.271018276762401e-07,
      "loss": 69.3597,
      "step": 2800
    },
    {
      "epoch": 0.7441253263707572,
      "grad_norm": 82.37113189697266,
      "learning_rate": 9.257963446475196e-07,
      "loss": 76.1725,
      "step": 2850
    },
    {
      "epoch": 0.7571801566579635,
      "grad_norm": 26.492717742919922,
      "learning_rate": 9.24490861618799e-07,
      "loss": 71.528,
      "step": 2900
    },
    {
      "epoch": 0.7702349869451697,
      "grad_norm": 64.44216918945312,
      "learning_rate": 9.231853785900783e-07,
      "loss": 74.5734,
      "step": 2950
    },
    {
      "epoch": 0.783289817232376,
      "grad_norm": 42.03432083129883,
      "learning_rate": 9.218798955613577e-07,
      "loss": 76.3158,
      "step": 3000
    },
    {
      "epoch": 0.7963446475195822,
      "grad_norm": 64.79264068603516,
      "learning_rate": 9.20574412532637e-07,
      "loss": 76.5058,
      "step": 3050
    },
    {
      "epoch": 0.8093994778067886,
      "grad_norm": 172.42979431152344,
      "learning_rate": 9.192689295039164e-07,
      "loss": 74.4521,
      "step": 3100
    },
    {
      "epoch": 0.8224543080939948,
      "grad_norm": 60.68756103515625,
      "learning_rate": 9.179634464751958e-07,
      "loss": 85.7919,
      "step": 3150
    },
    {
      "epoch": 0.835509138381201,
      "grad_norm": 62.882606506347656,
      "learning_rate": 9.166579634464751e-07,
      "loss": 88.8388,
      "step": 3200
    },
    {
      "epoch": 0.8485639686684073,
      "grad_norm": 158.72616577148438,
      "learning_rate": 9.153524804177545e-07,
      "loss": 96.1954,
      "step": 3250
    },
    {
      "epoch": 0.8616187989556136,
      "grad_norm": 75.72216796875,
      "learning_rate": 9.140469973890339e-07,
      "loss": 94.8691,
      "step": 3300
    },
    {
      "epoch": 0.8746736292428199,
      "grad_norm": 59.13572692871094,
      "learning_rate": 9.127415143603133e-07,
      "loss": 92.2933,
      "step": 3350
    },
    {
      "epoch": 0.8877284595300261,
      "grad_norm": 27.656858444213867,
      "learning_rate": 9.114360313315927e-07,
      "loss": 99.2612,
      "step": 3400
    },
    {
      "epoch": 0.9007832898172323,
      "grad_norm": 71.31889343261719,
      "learning_rate": 9.10130548302872e-07,
      "loss": 92.9095,
      "step": 3450
    },
    {
      "epoch": 0.9138381201044387,
      "grad_norm": 36.59492111206055,
      "learning_rate": 9.088250652741514e-07,
      "loss": 85.3119,
      "step": 3500
    },
    {
      "epoch": 0.9268929503916449,
      "grad_norm": 59.189212799072266,
      "learning_rate": 9.075195822454307e-07,
      "loss": 77.248,
      "step": 3550
    },
    {
      "epoch": 0.9399477806788512,
      "grad_norm": 73.46232604980469,
      "learning_rate": 9.062140992167101e-07,
      "loss": 82.6696,
      "step": 3600
    },
    {
      "epoch": 0.9530026109660574,
      "grad_norm": 53.935420989990234,
      "learning_rate": 9.049086161879896e-07,
      "loss": 84.8744,
      "step": 3650
    },
    {
      "epoch": 0.9660574412532638,
      "grad_norm": 57.64325714111328,
      "learning_rate": 9.036031331592688e-07,
      "loss": 83.3832,
      "step": 3700
    },
    {
      "epoch": 0.97911227154047,
      "grad_norm": 64.81415557861328,
      "learning_rate": 9.022976501305483e-07,
      "loss": 93.4951,
      "step": 3750
    },
    {
      "epoch": 0.9921671018276762,
      "grad_norm": 73.09991455078125,
      "learning_rate": 9.009921671018276e-07,
      "loss": 86.2246,
      "step": 3800
    },
    {
      "epoch": 1.0052219321148825,
      "grad_norm": 49.18534469604492,
      "learning_rate": 8.99686684073107e-07,
      "loss": 89.9726,
      "step": 3850
    },
    {
      "epoch": 1.0182767624020888,
      "grad_norm": 102.44544219970703,
      "learning_rate": 8.984073107049607e-07,
      "loss": 89.5629,
      "step": 3900
    },
    {
      "epoch": 1.031331592689295,
      "grad_norm": 42.37635040283203,
      "learning_rate": 8.971018276762402e-07,
      "loss": 90.3407,
      "step": 3950
    },
    {
      "epoch": 1.0443864229765012,
      "grad_norm": 40.4156608581543,
      "learning_rate": 8.957963446475196e-07,
      "loss": 86.0721,
      "step": 4000
    },
    {
      "epoch": 1.0574412532637076,
      "grad_norm": 48.50599670410156,
      "learning_rate": 8.944908616187989e-07,
      "loss": 80.6809,
      "step": 4050
    },
    {
      "epoch": 1.0704960835509139,
      "grad_norm": 61.3182487487793,
      "learning_rate": 8.931853785900783e-07,
      "loss": 76.4392,
      "step": 4100
    },
    {
      "epoch": 1.08355091383812,
      "grad_norm": 44.215309143066406,
      "learning_rate": 8.918798955613576e-07,
      "loss": 80.3339,
      "step": 4150
    },
    {
      "epoch": 1.0966057441253263,
      "grad_norm": 121.60108184814453,
      "learning_rate": 8.90574412532637e-07,
      "loss": 78.3696,
      "step": 4200
    },
    {
      "epoch": 1.1096605744125327,
      "grad_norm": 37.81155014038086,
      "learning_rate": 8.892689295039164e-07,
      "loss": 76.9744,
      "step": 4250
    },
    {
      "epoch": 1.122715404699739,
      "grad_norm": 60.411746978759766,
      "learning_rate": 8.879634464751958e-07,
      "loss": 81.5828,
      "step": 4300
    },
    {
      "epoch": 1.1357702349869452,
      "grad_norm": 91.86331939697266,
      "learning_rate": 8.866579634464752e-07,
      "loss": 79.83,
      "step": 4350
    },
    {
      "epoch": 1.1488250652741514,
      "grad_norm": 51.05310821533203,
      "learning_rate": 8.853524804177546e-07,
      "loss": 82.1695,
      "step": 4400
    },
    {
      "epoch": 1.1618798955613576,
      "grad_norm": 76.17620086669922,
      "learning_rate": 8.840469973890339e-07,
      "loss": 82.3342,
      "step": 4450
    },
    {
      "epoch": 1.174934725848564,
      "grad_norm": 83.31059265136719,
      "learning_rate": 8.827415143603133e-07,
      "loss": 79.9277,
      "step": 4500
    },
    {
      "epoch": 1.1879895561357703,
      "grad_norm": 71.56669616699219,
      "learning_rate": 8.814621409921671e-07,
      "loss": 83.9838,
      "step": 4550
    },
    {
      "epoch": 1.2010443864229765,
      "grad_norm": 54.790653228759766,
      "learning_rate": 8.801566579634464e-07,
      "loss": 75.4375,
      "step": 4600
    },
    {
      "epoch": 1.2140992167101827,
      "grad_norm": 36.18453598022461,
      "learning_rate": 8.788511749347258e-07,
      "loss": 77.6005,
      "step": 4650
    },
    {
      "epoch": 1.227154046997389,
      "grad_norm": 43.552772521972656,
      "learning_rate": 8.775456919060052e-07,
      "loss": 83.7673,
      "step": 4700
    },
    {
      "epoch": 1.2402088772845954,
      "grad_norm": 104.73970794677734,
      "learning_rate": 8.762402088772845e-07,
      "loss": 77.1097,
      "step": 4750
    },
    {
      "epoch": 1.2532637075718016,
      "grad_norm": 22.292354583740234,
      "learning_rate": 8.74934725848564e-07,
      "loss": 81.782,
      "step": 4800
    },
    {
      "epoch": 1.2663185378590078,
      "grad_norm": 41.05918884277344,
      "learning_rate": 8.736292428198433e-07,
      "loss": 78.8892,
      "step": 4850
    },
    {
      "epoch": 1.279373368146214,
      "grad_norm": 42.39561462402344,
      "learning_rate": 8.723237597911227e-07,
      "loss": 78.5848,
      "step": 4900
    },
    {
      "epoch": 1.2924281984334205,
      "grad_norm": 95.00761413574219,
      "learning_rate": 8.71018276762402e-07,
      "loss": 78.5677,
      "step": 4950
    },
    {
      "epoch": 1.3054830287206267,
      "grad_norm": 42.32379913330078,
      "learning_rate": 8.697127937336815e-07,
      "loss": 83.7209,
      "step": 5000
    },
    {
      "epoch": 1.318537859007833,
      "grad_norm": 49.52364730834961,
      "learning_rate": 8.684073107049607e-07,
      "loss": 81.5407,
      "step": 5050
    },
    {
      "epoch": 1.3315926892950392,
      "grad_norm": 46.30223846435547,
      "learning_rate": 8.671018276762402e-07,
      "loss": 81.5994,
      "step": 5100
    },
    {
      "epoch": 1.3446475195822454,
      "grad_norm": 66.98702239990234,
      "learning_rate": 8.657963446475195e-07,
      "loss": 74.5589,
      "step": 5150
    },
    {
      "epoch": 1.3577023498694518,
      "grad_norm": 50.461368560791016,
      "learning_rate": 8.644908616187989e-07,
      "loss": 83.2304,
      "step": 5200
    },
    {
      "epoch": 1.370757180156658,
      "grad_norm": 209.59349060058594,
      "learning_rate": 8.631853785900784e-07,
      "loss": 77.2396,
      "step": 5250
    },
    {
      "epoch": 1.3838120104438643,
      "grad_norm": 68.8273696899414,
      "learning_rate": 8.618798955613576e-07,
      "loss": 71.4629,
      "step": 5300
    },
    {
      "epoch": 1.3968668407310705,
      "grad_norm": 40.19097900390625,
      "learning_rate": 8.605744125326371e-07,
      "loss": 76.2916,
      "step": 5350
    },
    {
      "epoch": 1.4099216710182767,
      "grad_norm": 52.03288650512695,
      "learning_rate": 8.592689295039163e-07,
      "loss": 79.354,
      "step": 5400
    },
    {
      "epoch": 1.4229765013054831,
      "grad_norm": 37.13362121582031,
      "learning_rate": 8.579634464751958e-07,
      "loss": 80.2772,
      "step": 5450
    },
    {
      "epoch": 1.4360313315926894,
      "grad_norm": 66.22407531738281,
      "learning_rate": 8.566579634464752e-07,
      "loss": 83.1379,
      "step": 5500
    }
  ],
  "logging_steps": 50,
  "max_steps": 38300,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 536009278464000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
