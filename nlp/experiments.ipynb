{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa61729-d9b5-411f-b7c0-af51757b267f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./venv/lib/python3.10/site-packages (0.0.225)\n",
      "Requirement already satisfied: langchain-community in ./venv/lib/python3.10/site-packages (0.0.38)\n",
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-0.0.1-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.10/site-packages (4.41.0)\n",
      "Requirement already satisfied: accelerate in ./venv/lib/python3.10/site-packages (0.30.1)\n",
      "Requirement already satisfied: bitsandbytes in ./venv/lib/python3.10/site-packages (0.43.1)\n",
      "Requirement already satisfied: pydantic<2,>=1 in ./venv/lib/python3.10/site-packages (from langchain) (1.10.15)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./venv/lib/python3.10/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./venv/lib/python3.10/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: langchainplus-sdk<0.0.21,>=0.0.20 in ./venv/lib/python3.10/site-packages (from langchain) (0.0.20)\n",
      "Requirement already satisfied: requests<3,>=2 in ./venv/lib/python3.10/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./venv/lib/python3.10/site-packages (from langchain) (2.0.30)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in ./venv/lib/python3.10/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in ./venv/lib/python3.10/site-packages (from langchain) (2.10.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./venv/lib/python3.10/site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in ./venv/lib/python3.10/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./venv/lib/python3.10/site-packages (from langchain) (8.3.0)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in ./venv/lib/python3.10/site-packages (from langchain) (0.5.14)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.52 in ./venv/lib/python3.10/site-packages (from langchain-community) (0.1.52)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./venv/lib/python3.10/site-packages (from langchain-community) (0.1.59)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./venv/lib/python3.10/site-packages (from langchain-huggingface) (0.23.0)\n",
      "Collecting sentence-transformers>=2.6.0\n",
      "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting text-generation<0.8.0,>=0.7.0\n",
      "  Downloading text_generation-0.7.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in ./venv/lib/python3.10/site-packages (from langchain-huggingface) (0.19.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./venv/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./venv/lib/python3.10/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: psutil in ./venv/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./venv/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./venv/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.21.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.11.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2024.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./venv/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.52->langchain-community) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./venv/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.7)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.0)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.10/site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./venv/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "INFO: pip is looking at multiple versions of tenacity to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Using cached tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
      "INFO: pip is looking at multiple versions of sqlalchemy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Using cached SQLAlchemy-2.0.30-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "INFO: pip is looking at multiple versions of sentence-transformers to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sentence-transformers>=2.6.0\n",
      "  Downloading sentence_transformers-2.6.1-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.3/163.3 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of safetensors to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests<3,>=2\n",
      "  Downloading requests-2.32.2-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of regex to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "INFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting PyYAML>=5.4.1\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "INFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydantic<2,>=1\n",
      "  Using cached pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m651.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of pydantic to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading pydantic-1.10.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading pydantic-1.10.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25h  Downloading pydantic-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of packaging to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting packaging>=20.0\n",
      "  Using cached packaging-23.2-py3-none-any.whl (53 kB)\n",
      "INFO: pip is looking at multiple versions of openapi-schema-pydantic to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting openapi-schema-pydantic<2.0,>=1.2\n",
      "  Using cached openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
      "  Downloading openapi_schema_pydantic-1.2.3-py3-none-any.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading openapi_schema_pydantic-1.2.2-py3-none-any.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading openapi_schema_pydantic-1.2.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading openapi_schema_pydantic-1.2.0-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.7/81.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic<2,>=1\n",
      "  Downloading pydantic-1.8.1-py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.8-py3-none-any.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.5/124.5 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.7.4-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.7.3-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.7/107.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.7.2-py3-none-any.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.1/107.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.7.1-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.9/106.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.7-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.6.2-py36.py37.py38-none-any.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.3/99.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.6.1-py36.py37.py38-none-any.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.6-py36.py37.py38-none-any.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pydantic-1.5.1-py36.py37.py38-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy<2,>=1\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "INFO: pip is looking at multiple versions of openapi-schema-pydantic to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of numexpr to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numexpr<3.0.0,>=2.8.4\n",
      "  Using cached numexpr-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (376 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "INFO: pip is looking at multiple versions of langsmith to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langsmith<0.2.0,>=0.1.0\n",
      "  Downloading langsmith-0.1.60-py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Using cached langsmith-0.1.59-py3-none-any.whl (121 kB)\n",
      "  Downloading langsmith-0.1.58-py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.57-py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.56-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.8/120.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.55-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.54-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of langsmith to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading langsmith-0.1.53-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.52-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.51-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.0/116.0 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.50-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.49-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached langsmith-0.1.48-py3-none-any.whl (113 kB)\n",
      "  Downloading langsmith-0.1.47-py3-none-any.whl (113 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.0/113.0 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.46-py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.5/111.5 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.45-py3-none-any.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.44-py3-none-any.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.2/104.2 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.43-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.42-py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.41-py3-none-any.whl (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.7/91.7 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Using cached langsmith-0.1.40-py3-none-any.whl (87 kB)\n",
      "  Downloading langsmith-0.1.39-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.38-py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.37-py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.36-py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.35-py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.34-py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.33-py3-none-any.whl (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.6/86.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.31-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.30-py3-none-any.whl (71 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.29-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.28-py3-none-any.whl (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.27-py3-none-any.whl (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.26-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.25-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.24-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.23-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.22-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.21-py3-none-any.whl (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.20-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.19-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.18-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.17-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.16-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.15-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.8/64.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.14-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.13-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.12-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.2/63.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.11-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.10-py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.9-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.8-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.7-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.5-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.4-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.3-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.2-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.1-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading langsmith-0.1.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of langchainplus-sdk to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchainplus-sdk<0.0.21,>=0.0.20\n",
      "  Using cached langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-core<0.2.0,>=0.1.52\n",
      "  Using cached langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.23.0\n",
      "  Downloading huggingface_hub-0.23.1-py3-none-any.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.3/401.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain langchain-community langchain-huggingface transformers accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1fcaf5d-4eaf-434f-bbf2-11d4b4323704",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jon/Code/hackathon/til-24-base/nlp/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# To get environment variables\n",
    "import os\n",
    "import json\n",
    "\n",
    "# from rich.pretty import pprint\n",
    "\n",
    "\n",
    "# To split our transcript into pieces\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Our chat model. We'll use the default which is gpt-3.5-turbo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# from langchain_community.llms import HuggingFacePipeline\n",
    "# from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# Prompt templates for dynamic values\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate, # I included this one so you know you'll have it but we won't be using it\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "# To create our chat messages\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompt_values import StringPromptValue\n",
    "from typing import List, Optional\n",
    "\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35d058a6-29dc-4c15-bd4e-dfc201acde5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.90s/it]\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.76s/it]\n",
      "/home/jon/Code/hackathon/til-24-base/nlp/venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the from rom langchain-huggingface package and should be used instead. To use it run `pip install -U from rom langchain-huggingface` and import as `from from rom langchain_huggingface import llms import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                 model_id,\n",
    "                 quantization_config=nf4_config\n",
    "                 )\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                 model_id,\n",
    "                 quantization_config=nf4_config\n",
    "                 )\n",
    "\n",
    "# tokenizer.save_pretrained('./src/models/tokenizer')\n",
    "# model.save_pretrained('./src/models/model')\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "               model=model,\n",
    "               tokenizer=tokenizer,\n",
    "               max_new_tokens=512,\n",
    "               )\n",
    "\n",
    "llm_phi3 = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# llm_phi3 = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "#     task=\"text-generation\",\n",
    "#     device=0,\n",
    "#     pipeline_kwargs={\"max_new_tokens\": 512, \"temperature\": 0.1,},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061536c7-e6ea-4555-88de-c55bb5cd0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the transcript delimitted by triple backticks,\n",
    "extract all key information in a structured format.\n",
    "The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"description\": \"The data to be extracted from the transcript\", \"properties\": {\"target\": {\"title\": \"Target\", \"description\": \"Identified Target object to attack\", \"type\": \"string\"}, \"heading\": {\"title\": \"Heading\", \"description\": \"Heading of object in integers\", \"type\": \"string\"}, \"tool\": {\"title\": \"Tool\", \"description\": \"Tool to be deployed to neutralize the target\", \"type\": \"string\"}}, \"required\": [\"target\", \"heading\", \"tool\"]}\\n```\n",
    "Return the JSON object only, nothing else.\n",
    "\n",
    "Here are some examples of how to perform this task:\n",
    "TRANSCRIPT: Target is red helicopter, heading is zero one zero, tool to deploy is surface-to-air missiles.\n",
    "OUTPUT: 'target': 'red helicopter', 'heading': '010', 'tool': 'surface-to-air missiles'\n",
    "\n",
    "TRANSCRIPT: Target is grey and white fighter jet, heading is zero six five, tool to deploy is electromagnetic pulse.\n",
    "OUTPUT: 'target': 'grey and white fighter jet', 'heading': '065', 'tool': 'electromagnetic pulse'\n",
    "\n",
    "TRANSCRIPT: Alfa, Echo, Mike Papa, deploy EMP tool heading zero eight five, engage purple, red, and silver fighter jet.\n",
    "OUTPUT: 'target': 'purple, red, and silver fighter jet', 'heading': '085', 'tool': 'EMP'\n",
    "\n",
    "Now take the following transcript and extract the key information in a structured format\n",
    "TRANSCRIPT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c072c080-f3d5-4192-b833-9620ab948a45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcript = \"Target is blue stealth bomber, heading is zero five six, tool to deploy is cyber attack\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d7c906-b02d-49d2-a882-4164af44bf1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_json_string(json_str: str) -> str:\n",
    "    # Find the start of the first JSON object\n",
    "    start = json_str.find('{')\n",
    "\n",
    "    # Find the end of the first JSON object\n",
    "    end = json_str.find('}') + 1\n",
    "\n",
    "    # Extract the substring from 'start' to 'end'\n",
    "    cleaned_json_str = json_str[start:end]\n",
    "\n",
    "    return cleaned_json_str\n",
    "\n",
    "def extract_json_after_response(message: str) -> dict:\n",
    "    # Find the start of the JSON object\n",
    "    start = message.lower().find('response') + len('response')\n",
    "\n",
    "    # Extract the substring from 'start' to the end of the message\n",
    "    json_str = message[start:].strip()\n",
    "    json_str = clean_json_string(json_str)\n",
    "\n",
    "    try:\n",
    "        # Parse the JSON string into a Python dictionary\n",
    "        data = json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing JSON string: {json_str}\")\n",
    "        # raise\n",
    "        return None\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f8a7782-3ee4-4052-886e-083d84100a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nGiven the transcript delimitted by triple backticks,\\nextract all key information in a structured format.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"description\": \"The data to be extracted from the transcript\", \"properties\": {\"target\": {\"title\": \"Target\", \"description\": \"Identified Target object to attack\", \"type\": \"string\"}, \"heading\": {\"title\": \"Heading\", \"description\": \"Heading of object in integers\", \"type\": \"string\"}, \"tool\": {\"title\": \"Tool\", \"description\": \"Tool to be deployed to neutralize the target\", \"type\": \"string\"}}, \"required\": [\"target\", \"heading\", \"tool\"]}\\n```\\nReturn the JSON object only, nothing else.\\n\\nHere are some examples of how to perform this task:\\nTRANSCRIPT: Target is red helicopter, heading is zero one zero, tool to deploy is surface-to-air missiles.\\nOUTPUT: \\'target\\': \\'red helicopter\\', \\'heading\\': \\'010\\', \\'tool\\': \\'surface-to-air missiles\\'\\n\\nTRANSCRIPT: Target is grey and white fighter jet, heading is zero six five, tool to deploy is electromagnetic pulse.\\nOUTPUT: \\'target\\': \\'grey and white fighter jet\\', \\'heading\\': \\'065\\', \\'tool\\': \\'electromagnetic pulse\\'\\n\\nTRANSCRIPT: Alfa, Echo, Mike Papa, deploy EMP tool heading zero eight five, engage purple, red, and silver fighter jet.\\nOUTPUT: \\'target\\': \\'purple, red, and silver fighter jet\\', \\'heading\\': \\'085\\', \\'tool\\': \\'EMP\\'\\n\\nNow take the following transcript and extract the key information in a structured format\\nTRANSCRIPT:\\n```Target is blue stealth bomber, heading is zero five six, tool to deploy is cyber attack```\\n\\n\\n- response: {\\n  \"target\": \"blue stealth bomber\",\\n  \"heading\": \"056\",\\n  \"tool\": \"cyber attack\"\\n}'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = llm_phi3.predict(f\"{template}```{transcript}```\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00317a11-0141-443d-a8dc-e57c112255bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 'blue stealth bomber', 'heading': '056', 'tool': 'cyber attack'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_json_after_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc2f8202-f6be-4c81-8f5f-63659d4b814c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given the transcript delimitted by triple backticks,\n",
      "extract all key information in a structured format.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"description\": \"The data to be extracted from the transcript\", \"properties\": {\"target\": {\"title\": \"Target\", \"description\": \"Identified Target object to attack\", \"type\": \"string\"}, \"heading\": {\"title\": \"Heading\", \"description\": \"Heading of object in integers\", \"type\": \"string\"}, \"tool\": {\"title\": \"Tool\", \"description\": \"Tool to be deployed to neutralize the target\", \"type\": \"string\"}}, \"required\": [\"target\", \"heading\", \"tool\"]}\n",
      "```\n",
      "Return the JSON object only, nothing else.\n",
      "\n",
      "Here are some examples of how to perform this task:\n",
      "TRANSCRIPT: Target is red helicopter, heading is zero one zero, tool to deploy is surface-to-air missiles.\n",
      "OUTPUT: 'target': 'red helicopter', 'heading': '010', 'tool': 'surface-to-air missiles'\n",
      "\n",
      "TRANSCRIPT: Target is grey and white fighter jet, heading is zero six five, tool to deploy is electromagnetic pulse.\n",
      "OUTPUT: 'target': 'grey and white fighter jet', 'heading': '065', 'tool': 'electromagnetic pulse'\n",
      "\n",
      "TRANSCRIPT: Alfa, Echo, Mike Papa, deploy EMP tool heading zero eight five, engage purple, red, and silver fighter jet.\n",
      "OUTPUT: 'target': 'purple, red, and silver fighter jet', 'heading': '085', 'tool': 'EMP'\n",
      "\n",
      "Now take the following transcript and extract the key information in a structured format\n",
      "TRANSCRIPT:\n",
      "```Target is blue stealth bomber, heading is zero five six, tool to deploy is cyber attack```\n"
     ]
    }
   ],
   "source": [
    "print(f\"{template}```{transcript}```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f811cd1f-0780-4561-aba6-f1c4ba69583c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def load_test_cases(filename: str) -> list:\n",
    "  test_cases = []\n",
    "  with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "      # Each line is a separate JSON object\n",
    "      test_case = json.loads(line)\n",
    "      test_cases.append(test_case)\n",
    "  return test_cases\n",
    "\n",
    "# Load the test cases from the JSONL file\n",
    "test_cases = load_test_cases('./src/nlp.jsonl')\n",
    "print(f\"Loaded {len(test_cases)} test cases.\")\n",
    "\n",
    "# Counter for passed test cases\n",
    "passed_tests = 0\n",
    "\n",
    "# Now you can use 'test_cases' in your tests\n",
    "for test_case in tqdm(test_cases):\n",
    "  transcript = test_case['transcript']\n",
    "  expected_output = {\n",
    "    'target': test_case['target'],\n",
    "    'heading': test_case['heading'],\n",
    "    'tool': test_case['tool']\n",
    "  }\n",
    "  # Check if transcript is an instance of StringPromptValue and extract the string value\n",
    "  if isinstance(transcript, StringPromptValue):\n",
    "    print(transcript)\n",
    "    transcript = transcript.value\n",
    "\n",
    "  # Invoke the chain with the transcript from the test case\n",
    "  extracted_output = llm_phi3.predict(f\"{template}```{transcript}```\")\n",
    "  extracted_output = extract_json_after_response(extracted_output)\n",
    "\n",
    "  # Check if the extracted output matches the expected output\n",
    "  if extracted_output == expected_output:\n",
    "    passed_tests += 1\n",
    "  else:\n",
    "    print(f\"For transcript: {transcript}, expected: {expected_output}, but got: {extracted_output}\")\n",
    "\n",
    "# Print the number of passed tests\n",
    "print(f\"Passed {passed_tests} out of {len(test_cases)} tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6db5c-5d63-4328-9414-a918275e13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4f150-9017-4899-a499-f115f6099f36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Given the transcript delimitted by triple backticks,\n",
    "extract all key information in a structured format.\n",
    "{format_instructions}\n",
    "Return the JSON object only, nothing else.\n",
    "\n",
    "Here are some examples of how to perform this task:\n",
    "TRANSCRIPT: Target is red helicopter, heading is zero one zero, tool to deploy is surface-to-air missiles.\n",
    "OUTPUT: 'target': 'red helicopter', 'heading': '010', 'tool': 'surface-to-air missiles'\n",
    "\n",
    "TRANSCRIPT: Target is grey and white fighter jet, heading is zero six five, tool to deploy is electromagnetic pulse.\n",
    "OUTPUT: 'target': 'grey and white fighter jet', 'heading': '065', 'tool': 'electromagnetic pulse'\n",
    "\n",
    "TRANSCRIPT: Alfa, Echo, Mike Papa, deploy EMP tool heading zero eight five, engage purple, red, and silver fighter jet.\n",
    "OUTPUT: 'target': 'purple, red, and silver fighter jet', 'heading': '085', 'tool': 'EMP'\n",
    "\n",
    "Now take the following transcript and extract the key information in a structured format\n",
    "TRANSCRIPT:\n",
    "```{transcript}```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Objective(BaseModel):\n",
    "    \"\"\"The data to be extracted from the transcript\"\"\"\n",
    "    target: str = Field(description=\"Identified Target object to attack\")\n",
    "    heading: str = Field(description=\"Heading of object in integers\")\n",
    "    tool: str = Field(description=\"Tool to be deployed to neutralize the target\")\n",
    "\n",
    "def clean_json_string(json_str: str) -> str:\n",
    "    # Find the start of the first JSON object\n",
    "    start = json_str.find('{')\n",
    "\n",
    "    # Find the end of the first JSON object\n",
    "    end = json_str.find('}') + 1\n",
    "\n",
    "    # Extract the substring from 'start' to 'end'\n",
    "    cleaned_json_str = json_str[start:end]\n",
    "\n",
    "    return cleaned_json_str\n",
    "\n",
    "def extract_json_after_response(message: str) -> dict:\n",
    "    # Find the start of the JSON object\n",
    "    start = message.lower().find('response') + len('response')\n",
    "\n",
    "    # Extract the substring from 'start' to the end of the message\n",
    "    json_str = message[start:].strip()\n",
    "    json_str = clean_json_string(json_str)\n",
    "\n",
    "    try:\n",
    "        # Parse the JSON string into a Python dictionary\n",
    "        data = json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing JSON string: {json_str}\")\n",
    "        return None\n",
    "\n",
    "    return data\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Objective)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"transcript\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm_phi3 | extract_json_after_response\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def load_test_cases(filename: str) -> list:\n",
    "  test_cases = []\n",
    "  with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "      # Each line is a separate JSON object\n",
    "      test_case = json.loads(line)\n",
    "      test_cases.append(test_case)\n",
    "  return test_cases\n",
    "\n",
    "# Load the test cases from the JSONL file\n",
    "test_cases = load_test_cases('./data/nlp.jsonl')\n",
    "print(f\"Loaded {len(test_cases)} test cases.\")\n",
    "\n",
    "# Counter for passed test cases\n",
    "passed_tests = 0\n",
    "\n",
    "# Now you can use 'test_cases' in your tests\n",
    "for test_case in tqdm(test_cases):\n",
    "  transcript = test_case['transcript']\n",
    "  expected_output = {\n",
    "    'target': test_case['target'],\n",
    "    'heading': test_case['heading'],\n",
    "    'tool': test_case['tool']\n",
    "  }\n",
    "  # Check if transcript is an instance of StringPromptValue and extract the string value\n",
    "  if isinstance(transcript, StringPromptValue):\n",
    "    print(transcript)\n",
    "    transcript = transcript.value\n",
    "\n",
    "  # Invoke the chain with the transcript from the test case\n",
    "  extracted_output = chain.invoke({\"transcript\": transcript})\n",
    "\n",
    "\n",
    "  # Check if the extracted output matches the expected output\n",
    "  if extracted_output == expected_output:\n",
    "    passed_tests += 1\n",
    "  else:\n",
    "    print(f\"For transcript: {transcript}, expected: {expected_output}, but got: {extracted_output}\")\n",
    "\n",
    "# Print the number of passed tests\n",
    "print(f\"Passed {passed_tests} out of {len(test_cases)} tests.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1954b46-1cfc-45b3-8527-68bda3844c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3500 test cases.\n",
      "Test case 269 passed.\n",
      "Passed 1 out of 1 test.\n"
     ]
    }
   ],
   "source": [
    "### Test singular test case\n",
    "\n",
    "template = \"\"\"\n",
    "Given the transcript delimitted by triple backticks,\n",
    "extract all key information in a structured format.\n",
    "{format_instructions}\n",
    "Return the JSON object only, nothing else.\n",
    "\n",
    "Here are some examples of how to perform this task:\n",
    "TRANSCRIPT: Target is red helicopter, heading is zero one zero, tool to deploy is surface-to-air missiles.\n",
    "OUTPUT: 'target': 'red helicopter', 'heading': '010', 'tool': 'surface-to-air missiles'\n",
    "\n",
    "TRANSCRIPT: Target is grey and white fighter jet, heading is zero six five, tool to deploy is electromagnetic pulse.\n",
    "OUTPUT: 'target': 'grey and white fighter jet', 'heading': '065', 'tool': 'electromagnetic pulse'\n",
    "\n",
    "TRANSCRIPT: Alfa, Echo, Mike Papa, deploy EMP tool heading zero eight five, engage purple, red, and silver fighter jet.\n",
    "OUTPUT: 'target': 'purple, red, and silver fighter jet', 'heading': '085', 'tool': 'EMP'\n",
    "\n",
    "Now take the following transcript and extract the key information in a structured format\n",
    "TRANSCRIPT:\n",
    "```{transcript}```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Objective(BaseModel):\n",
    "    \"\"\"The data to be extracted from the transcript\"\"\"\n",
    "    target: str = Field(description=\"Identified Target object to attack\")\n",
    "    heading: str = Field(description=\"Heading of object in integers\")\n",
    "    tool: str = Field(description=\"Tool to be deployed to neutralize the target\")\n",
    "\n",
    "def clean_json_string(json_str: str) -> str:\n",
    "    # Find the start of the first JSON object\n",
    "    start = json_str.find('{')\n",
    "\n",
    "    # Find the end of the first JSON object\n",
    "    end = json_str.find('}') + 1\n",
    "\n",
    "    # Extract the substring from 'start' to 'end'\n",
    "    cleaned_json_str = json_str[start:end]\n",
    "\n",
    "    return cleaned_json_str\n",
    "\n",
    "def extract_json_after_response(message: str) -> dict:\n",
    "    # Find the start of the JSON object\n",
    "    start = message.lower().find('response') + len('response')\n",
    "\n",
    "    # Extract the substring from 'start' to the end of the message\n",
    "    json_str = message[start:].strip()\n",
    "    json_str = clean_json_string(json_str)\n",
    "\n",
    "    try:\n",
    "        # Parse the JSON string into a Python dictionary\n",
    "        data = json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing JSON string: {json_str}\")\n",
    "        return None\n",
    "\n",
    "    return data\n",
    "\n",
    "output_parser = JsonOutputParser(pydantic_object=Objective)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"transcript\"],\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | llm_phi3 | extract_json_after_response\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def load_test_cases(filename: str) -> list:\n",
    "  test_cases = []\n",
    "  with open(filename, 'r') as file:\n",
    "    for line in file:\n",
    "      # Each line is a separate JSON object\n",
    "      test_case = json.loads(line)\n",
    "      test_cases.append(test_case)\n",
    "  return test_cases\n",
    "\n",
    "# Load the test cases from the JSONL file\n",
    "test_cases = load_test_cases('./data/nlp.jsonl')\n",
    "print(f\"Loaded {len(test_cases)} test cases.\")\n",
    "\n",
    "# Counter for passed test cases\n",
    "passed_tests = 0\n",
    "\n",
    "# Get the 269th test case\n",
    "test_case = test_cases[269]\n",
    "\n",
    "transcript = test_case['transcript']\n",
    "expected_output = {\n",
    "  'target': test_case['target'],\n",
    "  'heading': test_case['heading'],\n",
    "  'tool': test_case['tool']\n",
    "}\n",
    "\n",
    "# Check if transcript is an instance of StringPromptValue and extract the string value\n",
    "if isinstance(transcript, StringPromptValue):\n",
    "  print(transcript)\n",
    "  transcript = transcript.value\n",
    "\n",
    "# Invoke the chain with the transcript from the test case\n",
    "extracted_output = chain.invoke({\"transcript\": transcript})\n",
    "\n",
    "# Check if the extracted output matches the expected output\n",
    "if extracted_output == expected_output:\n",
    "  passed_tests += 1\n",
    "  print(f\"Test case 269 passed.\")\n",
    "else:\n",
    "  print(f\"For transcript: {transcript}, expected: {expected_output}, but got: {extracted_output}\")\n",
    "\n",
    "# Print the number of passed tests\n",
    "print(f\"Passed {passed_tests} out of 1 test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c482c8a6-0b16-4be3-80df-9990b6aff33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 568\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('../results/nlp_results.csv')\n",
    "\n",
    "# Convert the 'truth' and 'prediction' columns from string to dictionary\n",
    "df['truth'] = df['truth'].apply(lambda x: json.loads(x.replace(\"'\", '\"')))\n",
    "df['prediction'] = df['prediction'].apply(lambda x: json.loads(x.replace(\"'\", '\"')))\n",
    "\n",
    "# Check how many predictions match the truth\n",
    "matches = sum(df['truth'] == df['prediction'])\n",
    "print(f'Number of matches: {matches}')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
